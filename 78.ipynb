{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40b0f724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\yg\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\yg\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: requests in c:\\users\\yg\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\yg\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yg\\anaconda3\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yg\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\yg\\anaconda3\\lib\\site-packages (from requests) (1.26.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install requests\n",
    "import sys\n",
    "import time \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "try:\n",
    "    \n",
    "    \n",
    "    page=requests.get('https://insights.blackcoffer.com/how-does-artificial-intelligence-affect-the-environment/')\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    error_type, error_obj, error_info = sys.exc_info()\n",
    "    print('Error for link :',url)\n",
    "    print(error_type,'Line :',error_info.tb_lineno)\n",
    "    \n",
    "    \n",
    "time.sleep(2)\n",
    "soup=BeautifulSoup(page.text,'html.parser')\n",
    "links=soup.find_all('h1',attrs={'class':'entry-title'})   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe56e9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does artificial intelligence affect the environment\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in links:\n",
    "    print(i.text)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a1eafec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "When we talk about AI, different people have different perspectives about it. There is one group of people, having good knowledge about the real potential of AI, who believe that AI can be a novel solution to many problems that the world is facing today. There is another group of people who are merely threatened by the thought of AI taking over the world.\n",
      "The field of AI took birth when Alan Turing in 1950 had this thought “Can machines think?”. Later in the 1980s, with the adoption of “expert systems” by the companies around the world, the booming of the field of AI was initiated. Initially, it was a matter of awe for everyone to see the results of what AI can achieve. AI’s growth in the past few decades has been exponential and it has transformed the way we live, work and solve challenges. AI has made its impact in a wide variety of areas including healthcare, education, business and many more. But, one of the areas of highest impact that AI has made recently is in the environment and climate change.\n",
      "Is AI Revolutionizing the way we deal with Environment and Climate Change?\n",
      "AI can strengthen climate predictions, enable smarter decision-making for decarbonising industries from building to transport, and work out how to allocate renewable energy. In recent years, AI has been a game-changer in how many people, as well as organizations, deal with climate change. Microsoft believes that artificial intelligence, often encompassing machine learning and deep learning, is a “game-changer” for climate change and environmental issues. The company’s ‘AI for Earth’ program has committed $50 million over five years to create and test new applications for AI.\n",
      "AI is increasingly used to manage the intermittency of renewable energy so that more can be incorporated into the grid; it can handle power fluctuations and improve energy storage as well. Wind companies are using AI to get each turbine’s propeller to produce more electricity per rotation by incorporating real-time weather and operational data.\n",
      "AI can also improve energy efficiency on the city scale by incorporating data from smart meters and the Internet of Things (the internet of computing devices that are embedded in everyday objects, enabling them to send and receive data) to forecast energy demand. Besides, artificial intelligence systems can simulate potential zoning laws, building ordinances, and flood plains to help with urban planning and disaster preparedness. One vision for a sustainable city is to create an “urban dashboard” consisting of real-time data on energy and water use and availability, traffic and weather to make cities more energy-efficient and livable.\n",
      "Hotter temperatures will have significant impacts on agriculture as well. Data from sensors in the field that monitor crop moisture, soil composition and temperature help AI improve production and know when crops need watering. Incorporating this information with that from drones, which are also used to monitor conditions, can help increasingly automatic AI systems know the best times to plant, spray and harvest crops, and when to head off diseases and other problems. This will result in increased efficiency, enhanced yields, and lower use of water, fertilizer and pesticides.\n",
      "But, is AI as good as it looks or does it have a downside?\n",
      "When we talk about the effect of AI in the environment and climate, both sides of the coin must be elucidated. So, apart from all the positives discussed above, the usage of AI also has a downside.\n",
      "Last year’s World Economic Forum report showed that while AI can address some of Earth’s environmental challenges, it is important to manage it properly. According to the forum and experts in the field, AI has the potential to accelerate environmental degradation. The use of power-intensive GPUs to run machine learning training has already been cited as contributing to increased C02 emissions.\n",
      "Although AI has been around for about half a century, the question of environmental impact – and other ethical issues – is only arising now because the techniques developed over decades can now be used in combination with an explosion in data and strong computational power.\n",
      "For all the advances enabled by artificial intelligence, from speech recognition to self-driving cars, AI systems consume a lot of power and can generate high volumes of climate-changing carbon emissions.\n",
      "A study last year found that training an off-the-shelf AI language-processing system produced 1,400 pounds of emissions – about the amount produced by flying one person roundtrip between New York and San Francisco. But there are ways to make machine learning cleaner and greener, a movement that has been called “Green AI.” Some algorithms are less power-hungry than others, for example, and many training sessions can be moved to remote locations that get most of their power from renewable sources.\n",
      "The key, however, is for AI developers and companies to know how much their machine learning experiments are spewing and how much those volumes could be reduced.\n",
      "What can be done?\n",
      "To prevent this, the proposition by the World Economic Forum report is that advancements in “safe” AI should be pursued, to ensure that humanity is not developing AI that is harmful to the environment. Specifically, the World Economic Forum said in its report that AI developers “must incorporate the health of the natural environment as a fundamental dimension.” This means safeguarding against models that will demand the consumption of energy or natural resources beyond what is sustainable, among other factors. In a sense, all programs need to be designed with the dimension of environmental protection and improvement in mind.\n",
      "In the future development of AI programs, it will also be important to note the environmental impact of creating these systems in the first place. According to an academic study on energy usage for deep learning processes, the creation of an effective AI might be costly to the environment. Nearly 300,000 kilograms of carbon dioxide equivalent emissions are created during the process of training a single model. This is basically equal to the emissions of five average cars in the United States. Considering the negative environmental impact in addition to the positive implications of AI and climate change will be crucial, moving forward.\n",
      "There is always room for improvement, some pioneers of machine learning recently published a paper called Tackling Climate Change with Machine Learning was discussed at a major AI conference. David Rolnick, a postdoctoral fellow of the University of Pennsylvania said “call to arms” which means to bring researchers together.\n",
      "This paper covers thirteen areas of research where machine learning can be used, including energy production, CO2 removal, education, solar geoengineering, finance and many more. The main idea is to build more energy-efficient buildings, creating new low-carbon materials, high tech monitoring of deforestation and greener transportation sources. Artificial intelligence and machine learning can be a medium but the real work is to be done by us as a caretaker of mother earth.\n",
      "How about better forecasting of climate change?\n",
      " The kick start was already done by climate informatics in 2011 which regulates the collaboration of data science and climate science. It covers a wide range of topics like improving prediction of extreme events such as hurricanes, paleoclimatology, collecting data from ice cores, climate up-down scaling and its impact on us.\n",
      "“There’s a lot of uncertainty,” Monteleoni\n",
      "Claire Monteleoni, a computer science professor at the University of Colorado concludes that AI/ ML models generally forecast for the short-term. There is a significant difference when it comes to long-term forecasting.\n",
      "The first system was developed at Princeton in the 1960s. All the AI/ ML models developed till now revolve around atmosphere, oceans, land, cryosphere or ice. AI can help to achieve new heights with the amount of data collected at this point of time, and compute more complex climate conditions using climate modeling algorithms.\n",
      "What about showing the effects of extreme weather?\n",
      "We all know that climate change is real and the rise in atmospheric temperature the proof, but to make it more realistic, Montreal Institute of Learning Algorithms (MILA), Microsoft and ConscientAI Labs uses Generative Adversarial Networks popularly know as GANs to simulate what mother earth will look like with the rising sea levels, atmospheric temperature, air pollution and intensive storms.\n",
      "“Our goal is not to convince people climate change is real, it’s to get people who do believe it is real to do more about that,” said Victor Schmidt, a co-author of the paper and Ph.D. candidate at MILA\n",
      "The project is deployed for the people to look at what their places will look like in the future when there will be a significant climate change.\n",
      "Can we track carbon emissions?\n",
      "Short answer, yes. Tracking of carbon emission is one of the agendas of the UN. For this year 2020, the UN’s goal is to prevent new coal plants from being built. Satellite images come into picture using deep learning techniques, data science researchers can analyze these high quality satellite images and track the emission. Google is expanding its horizon of nonprofit’s satellite imagery, including gas-powered plants emissions which can be used by researchers to track the pollution. There are countries organizations which govern CO2 emission on the ground level but these data are unreachable by global monitoring associations.\n",
      "AI can automate the analysis of images of power plants to get regular updates on emissions. It also introduces new ways to measure a plant’s impact, by crunching numbers of nearby infrastructure and electricity use. That’s beneficial for gas-powered plants that don’t have the easy-to-measure plumes that coal-powered plants have.\n",
      "Blackcoffer Insights 19: Shivendra Agarwal & Teena Mary (CHRIST University, Bangalore)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "links=soup.find_all('div',attrs={'class':'td-post-content tagdiv-type'})\n",
    "for i in links:\n",
    "    print(i.text)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e5de7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total positive score: 41\n"
     ]
    }
   ],
   "source": [
    "def count_positive_words(main_dataset_file, positive_words_file):\n",
    "    \n",
    "    \n",
    "\n",
    "    with open(main_dataset_file, 'r', encoding='utf-8') as main_file:\n",
    "        main_data = main_file.read()\n",
    "\n",
    "   \n",
    "    with open(positive_words_file, 'r', encoding='utf-8') as positive_file:\n",
    "        positive_words = positive_file.read().splitlines()\n",
    "\n",
    "    \n",
    "    words = main_data.split()\n",
    "\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "   \n",
    "    for word in words:\n",
    "      \n",
    "        if word in positive_words:\n",
    "            count += 1\n",
    "\n",
    "    \n",
    "    print(\"Total positive score:\", count)\n",
    "\n",
    "\n",
    "main_dataset_file = '78.txt'\n",
    "positive_words_file = 'positive.txt'\n",
    "\n",
    "\n",
    "count_positive_words(main_dataset_file, positive_words_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79bf2e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total negative score: 9\n"
     ]
    }
   ],
   "source": [
    "def count_positive_words(main_dataset_file, positive_words_file):\n",
    "    \n",
    "    with open(main_dataset_file, 'r', encoding='latin-1') as main_file:\n",
    "        main_data = main_file.read()\n",
    "\n",
    "   \n",
    "    with open(positive_words_file, 'r', encoding='latin-1') as positive_file:\n",
    "        positive_words = positive_file.read().splitlines()\n",
    "\n",
    "    \n",
    "    words = main_data.split()\n",
    "\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "   \n",
    "    for word in words:\n",
    "      \n",
    "        if word in positive_words:\n",
    "            count += 1\n",
    "\n",
    "    \n",
    "    print(\"Total negative score:\", count)\n",
    "\n",
    "\n",
    "main_dataset_file = '78.txt'\n",
    "positive_words_file = 'negative-words.txt'\n",
    "\n",
    "\n",
    "count_positive_words(main_dataset_file, positive_words_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41c19c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 1601\n"
     ]
    }
   ],
   "source": [
    "def count_total_words(dataset_file):\n",
    "   \n",
    "    with open(dataset_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    \n",
    "    words = content.split()\n",
    "\n",
    "    \n",
    "    total_words = len(words)\n",
    "\n",
    "    return total_words\n",
    "\n",
    "\n",
    "dataset_file = '78.txt'\n",
    "\n",
    "\n",
    "total_words = count_total_words(dataset_file)\n",
    "\n",
    "\n",
    "print(\"Total number of words:\", total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d36e461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of complex words: 212\n"
     ]
    }
   ],
   "source": [
    "import pyphen\n",
    "import re\n",
    "dataset_file = \"78.txt\"\n",
    "dictionary = pyphen.Pyphen(lang='en')\n",
    "def count_syllables(word):\n",
    "    return len(dictionary.inserted(word).split('-'))\n",
    "def count_complex_words(file):\n",
    "    complex_word_count = 0\n",
    "\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "        words = re.findall(r'\\b\\w+\\b', data)  \n",
    "\n",
    "        for word in words:\n",
    "            syllable_count = count_syllables(word)\n",
    "            if syllable_count > 2:\n",
    "                complex_word_count += 1\n",
    "\n",
    "    return complex_word_count\n",
    "complex_words_count = count_complex_words(dataset_file)\n",
    "print(\"Number of complex words:\", complex_words_count)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b47668c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 70\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def count_sentences(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', data)\n",
    "        sentence_count = len(sentences)\n",
    "\n",
    "    return sentence_count\n",
    "dataset_file = \"78.txt\"\n",
    "sentence_count = count_sentences(dataset_file)\n",
    "print(\"Number of sentences:\", sentence_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a323a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of total number of characters: 8461\n"
     ]
    }
   ],
   "source": [
    "def calculate_total_character_count(file):\n",
    "    total_character_count = 0\n",
    "\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "        words = data.split() \n",
    "\n",
    "        for word in words:\n",
    "            total_character_count += len(word)\n",
    "\n",
    "    return total_character_count\n",
    "dataset_file = \"78.txt\"\n",
    "character_count = calculate_total_character_count(dataset_file)\n",
    "print(\"Sum of total number of characters:\", character_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "831c3a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stop words present in data set: 739\n"
     ]
    }
   ],
   "source": [
    "def count_positive_words(main_dataset_file, positive_words_file):\n",
    "    \n",
    "    with open(main_dataset_file, 'r', encoding='utf-8') as main_file:\n",
    "        main_data = main_file.read()\n",
    "\n",
    "   \n",
    "    with open(positive_words_file, 'r', encoding='utf-8') as positive_file:\n",
    "        positive_words = positive_file.read().splitlines()\n",
    "\n",
    "    \n",
    "    words = main_data.split()\n",
    "\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "   \n",
    "    for word in words:\n",
    "      \n",
    "        if word in positive_words:\n",
    "            count += 1\n",
    "\n",
    "    \n",
    "    print(\"Total stop words present in data set:\", count)\n",
    "\n",
    "\n",
    "main_dataset_file = '78.txt'\n",
    "positive_words_file = 'all-stop-words.txt'\n",
    "\n",
    "\n",
    "count_positive_words(main_dataset_file, positive_words_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ace0b593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total syllables in the dataset file: 2694\n"
     ]
    }
   ],
   "source": [
    "def count_syllables(word):\n",
    "    vowels = 'aeiouAEIOU'\n",
    "    exceptions = ['es', 'ed']\n",
    "    count = 0\n",
    "    prev_char_vowel = False\n",
    "\n",
    "    if word[-2:] in exceptions:\n",
    "        return count\n",
    "\n",
    "    for char in word:\n",
    "        if char in vowels:\n",
    "            if not prev_char_vowel:\n",
    "                count += 1\n",
    "            prev_char_vowel = True\n",
    "        else:\n",
    "            prev_char_vowel = False\n",
    "\n",
    "    return count\n",
    "\n",
    "def sum_syllables_in_file(file):\n",
    "    total_syllables = 0\n",
    "\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "        words = data.split()\n",
    "\n",
    "        for word in words:\n",
    "            total_syllables += count_syllables(word)\n",
    "\n",
    "    return total_syllables\n",
    "\n",
    "dataset_file = \"78.txt\"\n",
    "total_syllables = sum_syllables_in_file(dataset_file)\n",
    "print(\"Total syllables in the dataset file:\", total_syllables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ca3e911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of personal pronouns mentioned: 5\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def count_personal_pronouns(file):\n",
    "    pronouns = ['I', 'we', 'my', 'ours', 'us']\n",
    "    count = 0\n",
    "\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "        words = re.findall(r'\\b(?:{})\\b'.format('|'.join(pronouns)), data)\n",
    "\n",
    "        for word in words:\n",
    "            # Exclude the word 'US' (country name)\n",
    "            if word.lower() != 'us':\n",
    "                count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "dataset_file = \"78.txt\"\n",
    "pronoun_count = count_personal_pronouns(dataset_file)\n",
    "print(\"Number of personal pronouns mentioned:\", pronoun_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d0af29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 8461\n",
      "Total words: 1601\n",
      "Average word length: 5.284821986258589\n"
     ]
    }
   ],
   "source": [
    "def calculate_word_stats(file):\n",
    "    total_characters = 0\n",
    "    total_words = 0\n",
    "\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "        words = data.split()\n",
    "\n",
    "        for word in words:\n",
    "            total_characters += len(word)\n",
    "            total_words += 1\n",
    "\n",
    "    return total_characters, total_words\n",
    "\n",
    "dataset_file = \"78.txt\"\n",
    "char_count, word_count = calculate_word_stats(dataset_file)\n",
    "average_word_length = char_count / word_count if word_count != 0 else 0\n",
    "\n",
    "print(\"Total characters:\", char_count)\n",
    "print(\"Total words:\", word_count)\n",
    "print(\"Average word length:\", average_word_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2cf6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7024b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
